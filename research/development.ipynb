{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dataset loading: \n",
      "Datasets ready to use!\n",
      "Total Length of DataFrame is 23145\n",
      "\n",
      " Study the choose_one_category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choose_one_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>affected_people</th>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caution_and_advice</th>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths_reports</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_signs_or_symptoms</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_transmission</th>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displaced_people_and_evacuations</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donation_needs_or_offers_or_volunteering_services</th>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_and_utilities_damage</th>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured_or_dead_people</th>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_trapped_or_found_people</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_related_or_irrelevant</th>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_useful_information</th>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevention</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sympathy_and_emotional_support</th>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   _unit_state  tweet_text  \\\n",
       "choose_one_category                                                          \n",
       "affected_people                                            866         866   \n",
       "caution_and_advice                                        1063        1063   \n",
       "deaths_reports                                              93          93   \n",
       "disease_signs_or_symptoms                                  431         431   \n",
       "disease_transmission                                       425         425   \n",
       "displaced_people_and_evacuations                           633         633   \n",
       "donation_needs_or_offers_or_volunteering_services         2610        2610   \n",
       "infrastructure_and_utilities_damage                       1851        1851   \n",
       "injured_or_dead_people                                    2510        2510   \n",
       "missing_trapped_or_found_people                            402         402   \n",
       "not_related_or_irrelevant                                 2598        2598   \n",
       "other_useful_information                                  6984        6984   \n",
       "prevention                                                 288         288   \n",
       "sympathy_and_emotional_support                            1969        1969   \n",
       "treatment                                                  422         422   \n",
       "\n",
       "                                                    cat  \n",
       "choose_one_category                                      \n",
       "affected_people                                     866  \n",
       "caution_and_advice                                 1063  \n",
       "deaths_reports                                       93  \n",
       "disease_signs_or_symptoms                           431  \n",
       "disease_transmission                                425  \n",
       "displaced_people_and_evacuations                    633  \n",
       "donation_needs_or_offers_or_volunteering_services  2610  \n",
       "infrastructure_and_utilities_damage                1851  \n",
       "injured_or_dead_people                             2510  \n",
       "missing_trapped_or_found_people                     402  \n",
       "not_related_or_irrelevant                          2598  \n",
       "other_useful_information                           6984  \n",
       "prevention                                          288  \n",
       "sympathy_and_emotional_support                     1969  \n",
       "treatment                                           422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Study the cat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>choose_one_category</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eq</th>\n",
       "      <td>9057</td>\n",
       "      <td>9057</td>\n",
       "      <td>9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flood</th>\n",
       "      <td>4016</td>\n",
       "      <td>4016</td>\n",
       "      <td>4016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>6039</td>\n",
       "      <td>6039</td>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <td>4033</td>\n",
       "      <td>4033</td>\n",
       "      <td>4033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_state  choose_one_category  tweet_text\n",
       "cat                                                \n",
       "eq            9057                 9057        9057\n",
       "flood         4016                 4016        4016\n",
       "storm         6039                 6039        6039\n",
       "virus         4033                 4033        4033"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example before cleaning at position 4\n",
      "Aid agencies: Vanuatu conditions more challenging than Philippines typhoon http://t.co/G9WIl4qrHx\n",
      "example after cleaning at position 4\n",
      "aid agencies  vanuatu conditions more challenging than philippines typhoon \n"
     ]
    }
   ],
   "source": [
    "# df_clean method default settings hashtag_just_sign=True, remove_stopwords=True, test_position=4\n",
    "df_clean = tweet_cleaner(df, remove_stopwords=False, test_position=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on content: 'other_useful_information', 'infrastructure_and_utilities_damage',\n",
    "       'injured_or_dead_people', 'not_related_or_irrelevant',\n",
    "       'donation_needs_or_offers_or_volunteering_services',\n",
    "       'caution_and_advice', 'sympathy_and_emotional_support',\n",
    "       'missing_trapped_or_found_people',\n",
    "       'displaced_people_and_evacuations', 'affected_people',\n",
    "       'disease_signs_or_symptoms', 'prevention', 'disease_transmission',\n",
    "       'treatment', 'deaths_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing ...\n",
      "                         \n",
      "Train forest for  other_useful_information\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9816915100453661 \n",
      " The validation accuracy is:  0.8006048822639878\n",
      "confusion matrix\n",
      "[[2959  265]\n",
      " [ 658  747]]\n",
      "check classes on which we trained\n",
      "['no_other_useful_information' 'other_useful_information']\n",
      "[0.00029742 0.00176676 0.00025415 ... 0.00042826 0.00036104 0.00039069]\n",
      "Feature ranking:\n",
      "['earthquake', 'in', 'california', 'of', 'to', 'the', 'dead', 'questions', 'for', 'symptoms', 'on', 'and', 'nepal', 'killed', 'help', 'is', 'disease', 'magnitude', 'people', 'hits', 'from', 'as', 'up', 'deadly', 'death', '10', 'via', 'news', 'island', 'northern', 'after', 'chile', 'relief', 'india', 'kashmir', 'damage', 'quake', '2014', 'injured', 'toll', 'my', 'by', 'flood', 'ebola', 'floods', 'that', 'cyclone', 'typhoon', 'about', 'at']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  infrastructure_and_utilities_damage\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.990548714625189 \n",
      " The validation accuracy is:  0.950097213220998\n",
      "confusion matrix\n",
      "[[ 224  152]\n",
      " [  79 4174]]\n",
      "check classes on which we trained\n",
      "['infrastructure_and_utilities_damage'\n",
      " 'no_infrastructure_and_utilities_damage']\n",
      "[2.50685568e-05 9.84556561e-04 3.67197845e-06 ... 2.21697797e-05\n",
      " 1.02763836e-04 2.15214570e-05]\n",
      "Feature ranking:\n",
      "['damage', 'destruction', 'hurricane', 'odile', 'damaged', 'damages', 'cabo', 'in', 'of', 'lucas', 'buildings', 'the', 'extensive', 'path', 'to', 'airport', 'hotels', 'seen', 'cabos', 'earthquake', 'from', 'homes', 'mexico', 'baja', 'via', 'news', 'power', 'and', 'california', 'napa', 'vanuatu', 'by', 'destroyed', 'after', 'losses', 'los', 'businesses', 'is', 'san', 'photos', 'instagram', 'causes', 'property', 'for', 'as', 'resorts', 'cabosanlucas', 'tragic', 'through', 'help']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  injured_or_dead_people\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9982177576150356 \n",
      " The validation accuracy is:  0.9773169151004537\n",
      "confusion matrix\n",
      "[[ 420   62]\n",
      " [  43 4104]]\n",
      "check classes on which we trained\n",
      "['injured_or_dead_people' 'no_injured_or_dead_people']\n",
      "[6.53772837e-05 2.93575228e-03 1.50284083e-04 ... 1.12414570e-04\n",
      " 5.40973420e-06 6.79266768e-05]\n",
      "Feature ranking:\n",
      "['dead', 'toll', 'killed', 'death', 'india', 'injured', 'least', 'dozens', 'nepal', 'in', 'kill', 'floods', 'deaths', 'hundreds', 'injuries', 'rises', '180', 'kills', 'died', 'people', 'monsoon', 'flooding', 'pakistan', 'the', 'and', 'as', 'many', 'of', 'scores', 'to', '160', 'earthquake', 'for', 'critically', '120', '200', 'lives', 'hurt', 'bodies', 'than', 'reaches', 'at', 'been', 'more', 'quake', 'nearly', 'killing', 'have', 'landslides', 'flood']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  not_related_or_irrelevant\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9956254050550875 \n",
      " The validation accuracy is:  0.9280622164614387\n",
      "confusion matrix\n",
      "[[4063   66]\n",
      " [ 267  233]]\n",
      "check classes on which we trained\n",
      "['no_not_related_or_irrelevant' 'not_related_or_irrelevant']\n",
      "[3.50984218e-04 7.76059100e-04 1.57975547e-04 ... 4.53770903e-04\n",
      " 2.66398779e-06 3.04261024e-05]\n",
      "Feature ranking:\n",
      "['up', 'pam', 'cyclone', 'in', 'me', 'you', 'earthquake', 'the', 'of', 'to', 'ebola', 'nepal', 'vanuatu', 'floods', 'for', 'is', 'my', 'and', 'jim', 'mers', 'flood', 'on', 'love', 'after', 'this', 'chile', 'odile', 'help', 'it', 'from', 'california', 'like', 'what', 'are', 'rubyph', 'pakistan', 'question', 'prayforchile', 'that', 'photo', 'as', 'so', 'quake', 'by', 'with', 'when', 'nepalearthquake', 'napa', 'typhoon', 'hurricane']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  donation_needs_or_offers_or_volunteering_services\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9976236768200475 \n",
      " The validation accuracy is:  0.937351479801253\n",
      "confusion matrix\n",
      "[[ 362  170]\n",
      " [ 120 3977]]\n",
      "check classes on which we trained\n",
      "['donation_needs_or_offers_or_volunteering_services'\n",
      " 'no_donation_needs_or_offers_or_volunteering_services']\n",
      "[5.72199789e-05 1.42065744e-03 2.77396148e-05 ... 1.57335996e-04\n",
      " 1.56850734e-04 1.43813702e-05]\n",
      "Feature ranking:\n",
      "['help', 'relief', 'aid', 'donate', 'victims', 'to', 'for', 'support', 'in', 'food', 'vanuatu', 'appeal', 'donations', 'nepal', 'the', 'assistance', 'send', 'of', 'rescue', 'need', 'goods', 'flood', 'efforts', 'fund', 'rs', 'and', 'donating', 'needs', 'nepalearthquake', 'teams', 'ebola', 'team', 'helping', 'supplies', 'donation', 'earthquake', 'kashmir', 'funds', 'mqm', 'is', 'please', 'from', 'work', 'on', 'reliefbykkf', 'india', 'we', 'army', 'volunteers', 'contribute']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  caution_and_advice\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9975696694750487 \n",
      " The validation accuracy is:  0.9591704471808166\n",
      "confusion matrix\n",
      "[[  54  160]\n",
      " [  29 4386]]\n",
      "check classes on which we trained\n",
      "['caution_and_advice' 'no_caution_and_advice']\n",
      "[1.08699970e-03 4.44547451e-04 7.45401903e-04 ... 3.79995868e-04\n",
      " 3.00431653e-04 7.19390209e-05]\n",
      "Feature ranking:\n",
      "['warning', 'alert', 'warnings', 'tsunami', 'the', 'of', 'in', 'rubyph', 'hagupit', 'chile', 'for', 'to', 'issued', 'expected', 'earthquake', 'advisory', 'typhoon', 'storm', 'and', 'peru', 'on', 'as', 'is', 'news', 'be', 'after', 'safe', 'via', 'from', 'advice', 'flood', 'watches', 'california', 'magnitude', 'system', 'coast', 'signal', 'quake', 'philippines', 'pagasa', 'stay', 'by', 'tropical', 'landfall', 'with', 'are', 'prepared', 'pakistan', 'metro', 'at']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  sympathy_and_emotional_support\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9981097429250378 \n",
      " The validation accuracy is:  0.9576582415208469\n",
      "confusion matrix\n",
      "[[4169   43]\n",
      " [ 153  264]]\n",
      "check classes on which we trained\n",
      "['no_sympathy_and_emotional_support' 'sympathy_and_emotional_support']\n",
      "[1.18089244e-04 3.75634340e-04 3.79877661e-05 ... 2.37785934e-06\n",
      " 2.26836754e-06 1.66936986e-05]\n",
      "Feature ranking:\n",
      "['prayers', 'pray', 'prayforchile', 'thoughts', 'god', 'praying', 'everyone', 'all', 'hope', 'our', 'my', 'safe', 'those', 'the', 'prayfornepal', 'for', 'to', 'in', 'and', 'keep', 'of', 'allah', 'prayer', 'chile', 'earthquake', 'heart', 'go', 'let', 'please', 'nepal', 'stay', 'condolences', 'affected', 'prayforthephilippines', 'with', 'lord', 'people', 'is', 'bless', 'are', 'out', 'them', 'by', 'on', 'you', 'rubyph', 'we', 'prayforsouthkorea', 'help', 'families']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  missing_trapped_or_found_people\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9989738604450206 \n",
      " The validation accuracy is:  0.9833657377403326\n",
      "confusion matrix\n",
      "[[  17   73]\n",
      " [   4 4535]]\n",
      "check classes on which we trained\n",
      "['missing_trapped_or_found_people' 'no_missing_trapped_or_found_people']\n",
      "[4.87462080e-04 4.04616548e-03 0.00000000e+00 ... 2.55958299e-05\n",
      " 2.18006959e-04 4.69939261e-04]\n",
      "Feature ranking:\n",
      "['missing', 'trapped', 'rescued', 'rubble', 'stranded', 'in', 'alive', 'nepal', 'baby', 'floods', 'to', 'the', 'kashmir', 'from', 'of', 'and', 'after', 'thousands', 'for', 'earthquake', 'still', 'pulled', 'people', 'india', 'rescue', 'is', 'news', 'nepalearthquake', 'flood', 'on', 'find', 'residents', 'rescues', 'nepalquake', 'old', 'are', 'pakistan', 'survivor', '000', 'help', 'quake', 'via', 'rescuers', 'kashmirfloods', 'hundreds', 'brother', 'days', 'under', 'hours', 'at']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  displaced_people_and_evacuations\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9985958090300281 \n",
      " The validation accuracy is:  0.9842298552603154\n",
      "confusion matrix\n",
      "[[  55   66]\n",
      " [   7 4501]]\n",
      "check classes on which we trained\n",
      "['displaced_people_and_evacuations' 'no_displaced_people_and_evacuations']\n",
      "[9.10103783e-06 6.82029554e-03 6.04636874e-07 ... 1.61074260e-04\n",
      " 4.46540295e-05 3.50239036e-04]\n",
      "Feature ranking:\n",
      "['evacuation', 'evacuated', 'flee', 'residents', 'tourists', 'shelters', 'thousands', 'evacuations', 'shelter', 'as', 'evacuate', 'in', 'sent', 'to', 'makes', 'evacuating', 'of', 'hurricane', 'landfall', 'holidaymakers', 'odile', 'abc', 'evacuees', 'the', '000', 'million', 'and', 'via', 'from', 'after', 'indians', 'for', 'philippines', 'india', 'by', 'flood', 'news', 'people', 'displaced', 'typhoon', 'hagupit', 'rubyph', 'nepal', 'floods', 'are', 'pakistan', 'earthquake', 'homeless', 'baja', 'more']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  affected_people\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9962194858500756 \n",
      " The validation accuracy is:  0.9853100021602939\n",
      "confusion matrix\n",
      "[[ 115   51]\n",
      " [  17 4446]]\n",
      "check classes on which we trained\n",
      "['affected_people' 'no_affected_people']\n",
      "[3.45018291e-07 4.90753110e-05 1.10616912e-06 ... 1.99732864e-06\n",
      " 1.02667451e-04 9.76161998e-07]\n",
      "Feature ranking:\n",
      "['case', 'mers', 'centers', 'disease', 'control', 'florida', 'symptoms', 'cases', 'finds', 'of', 'patient', 'in', 'deadly', 'first', 'the', 'confirmed', 'saudi', 'virus', 'arabia', 'cdc', 'for', 'middle', 'prevention', 'ebola', 'third', 'two', 'confirms', 'eastern', 'to', 'workers', 'show', 'another', 'spreads', 'reported', 'nurse', 'orlando', 'found', '18', 'health', 'as', 'indiana', 'and', 'with', 'says', 'us', 'on', '32', 'hospital', 'infected', 'via']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  disease_signs_or_symptoms\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9972456254050551 \n",
      " The validation accuracy is:  0.9887664722402246\n",
      "confusion matrix\n",
      "[[  43   43]\n",
      " [   9 4534]]\n",
      "check classes on which we trained\n",
      "['disease_signs_or_symptoms' 'no_disease_signs_or_symptoms']\n",
      "[3.66561370e-05 2.39131796e-04 3.96086548e-06 ... 4.07014319e-07\n",
      " 1.29451260e-05 9.55606848e-06]\n",
      "Feature ranking:\n",
      "['symptoms', 'ebola', 'patient', 'fever', 'vomiting', 'mers', 'signs', 'of', 'via', 'showing', 'like', 'flu', 'to', 'screen', 'in', 'hospital', 'abuja', 'with', 'monitored', 'for', 'disease', 'the', 'virus', 'show', 'canada', 'pain', 'tests', 'is', 'after', 'ohio', 'and', 'health', 'on', 'pupils', 'isolation', 'travel', 'treatment', 'florida', 'negative', 'at', 'cov', 'news', 'coronavirus', 'resume', 'has', 'being', 'ontario', 'have', 'starts', 'workers']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  prevention\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9988658457550227 \n",
      " The validation accuracy is:  0.9902786779001944\n",
      "confusion matrix\n",
      "[[4560    9]\n",
      " [  36   24]]\n",
      "check classes on which we trained\n",
      "['no_prevention' 'prevention']\n",
      "[5.66515494e-09 9.16614307e-05 0.00000000e+00 ... 3.18367726e-07\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "Feature ranking:\n",
      "['prevention', 'ebola', 'bath', 'guidelines', 'infection', 'prevent', 'mers', 'the', 'to', 'in', 'salt', 'disease', 'for', 'of', 'on', 'control', 'with', 'and', 'cure', 'centers', 'virus', 'protection', 'is', 'precautions', 'symptoms', 'cdc', 'water', 'health', 'measures', 'it', 'who', 'spread', 'committee', 'tips', 'via', 'stop', 'by', 'news', 'from', 'against', 'hospitalised', 'quarantine', 'fukuda', '20', 'best', 'issues', 'high', 'case', 'about', 'advice']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  disease_transmission\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9976236768200475 \n",
      " The validation accuracy is:  0.9898466191402031\n",
      "confusion matrix\n",
      "[[  51   34]\n",
      " [  13 4531]]\n",
      "check classes on which we trained\n",
      "['disease_transmission' 'no_disease_transmission']\n",
      "[7.89112086e-08 1.21018425e-04 3.25150076e-06 ... 0.00000000e+00\n",
      " 1.57111880e-06 0.00000000e+00]\n",
      "Feature ranking:\n",
      "['transmission', 'mers', 'spread', 'human', 'camel', 'ebola', 'disease', 'spreads', 'to', 'virus', 'evidence', 'airborne', 'respiratory', 'of', 'via', 'the', 'in', 'saudi', 'is', 'iran', 'cdc', 'and', 'by', 'on', 'camels', 'for', 'from', 'symptoms', 'nigeria', 'sustained', 'news', 'coronavirus', 'still', 'health', 'mecca', 'it', 'as', 'with', 'person', 'contagious', 'not', 'expert', 'be', 'can', 'infected', 'deadly', 'an', 'question', 'guinea', 'us']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  treatment\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.998649816375027 \n",
      " The validation accuracy is:  0.9898466191402031\n",
      "confusion matrix\n",
      "[[4530   16]\n",
      " [  31   52]]\n",
      "check classes on which we trained\n",
      "['no_treatment' 'treatment']\n",
      "[0.00000000e+00 4.11627794e-06 4.53171855e-04 ... 2.68380338e-05\n",
      " 4.24920592e-05 1.00890374e-03]\n",
      "Feature ranking:\n",
      "['treatment', 'ebola', 'treatments', 'experimental', 'drug', 'for', 'liberia', 'to', 'biotech', 'the', 'of', 'disease', 'in', 'symptoms', 'cure', 'mers', 'serum', 'specific', 'and', 'is', 'with', 'outbreak', 'patients', 'virus', 'hospital', 'new', 'use', 'flown', 'nurse', 'us', 'as', 'news', 'an', 'treated', 'vaccine', 'health', 'may', 'patient', 'on', 'no', 'who', 'that', 'from', 'canada', 'be', 'via', 'fear', 'atlanta', 'treating', 'two']\n",
      "testing ...\n",
      "                         \n",
      "Train forest for  deaths_reports\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9997299632750054 \n",
      " The validation accuracy is:  0.9978397062000433\n",
      "confusion matrix\n",
      "[[   4    8]\n",
      " [   2 4615]]\n",
      "check classes on which we trained\n",
      "['deaths_reports' 'no_deaths_reports']\n",
      "[1.25191005e-05 1.85108956e-03 0.00000000e+00 ... 7.25744449e-05\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "Feature ranking:\n",
      "['hospitalised', 'consumption', 'killed', 'high', 'dead', 'ebola', 'disease', 'deaths', 'pass', 'eight', 'as', 'news', 'arabia', 'the', 'over', 'in', 'from', 'salt', 'via', 'died', 'saudi', 'die', 'mers', 'death', 'doctor', 'africa', 'ht', 'who', '20', 'breaking', 'cases', 'to', 'deadly', 'prevention', 'almost', 'reports', 'of', 'by', '100', 'for', 'more', 'and', 'up', 'dies', 'since', 'has', 'on', '10', 'americans', 'health']\n",
      "DONE with Training on Content!!!\n"
     ]
    }
   ],
   "source": [
    "output_content, output_content_importances, output_content_feature_names = training_on_content(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on cat: 'storm', 'eq', 'flood', 'virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_category, output_cat_importances, output_cat_feature_names = training_on_cat(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_category)\n",
    "print(output_cat_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_cat_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other_useful_information', 'infrastructure_and_utilities_damage', 'injured_or_dead_people', 'not_related_or_irrelevant', 'donation_needs_or_offers_or_volunteering_services', 'caution_and_advice', 'sympathy_and_emotional_support', 'missing_trapped_or_found_people', 'displaced_people_and_evacuations', 'affected_people', 'disease_signs_or_symptoms', 'prevention', 'disease_transmission', 'treatment', 'deaths_reports']\n",
      "[array([0.00029742, 0.00176676, 0.00025415, ..., 0.00042826, 0.00036104,\n",
      "       0.00039069]), array([2.50685568e-05, 9.84556561e-04, 3.67197845e-06, ...,\n",
      "       2.21697797e-05, 1.02763836e-04, 2.15214570e-05]), array([6.53772837e-05, 2.93575228e-03, 1.50284083e-04, ...,\n",
      "       1.12414570e-04, 5.40973420e-06, 6.79266768e-05]), array([3.50984218e-04, 7.76059100e-04, 1.57975547e-04, ...,\n",
      "       4.53770903e-04, 2.66398779e-06, 3.04261024e-05]), array([5.72199789e-05, 1.42065744e-03, 2.77396148e-05, ...,\n",
      "       1.57335996e-04, 1.56850734e-04, 1.43813702e-05]), array([1.08699970e-03, 4.44547451e-04, 7.45401903e-04, ...,\n",
      "       3.79995868e-04, 3.00431653e-04, 7.19390209e-05]), array([1.18089244e-04, 3.75634340e-04, 3.79877661e-05, ...,\n",
      "       2.37785934e-06, 2.26836754e-06, 1.66936986e-05]), array([4.87462080e-04, 4.04616548e-03, 0.00000000e+00, ...,\n",
      "       2.55958299e-05, 2.18006959e-04, 4.69939261e-04]), array([9.10103783e-06, 6.82029554e-03, 6.04636874e-07, ...,\n",
      "       1.61074260e-04, 4.46540295e-05, 3.50239036e-04]), array([3.45018291e-07, 4.90753110e-05, 1.10616912e-06, ...,\n",
      "       1.99732864e-06, 1.02667451e-04, 9.76161998e-07]), array([3.66561370e-05, 2.39131796e-04, 3.96086548e-06, ...,\n",
      "       4.07014319e-07, 1.29451260e-05, 9.55606848e-06]), array([5.66515494e-09, 9.16614307e-05, 0.00000000e+00, ...,\n",
      "       3.18367726e-07, 0.00000000e+00, 0.00000000e+00]), array([7.89112086e-08, 1.21018425e-04, 3.25150076e-06, ...,\n",
      "       0.00000000e+00, 1.57111880e-06, 0.00000000e+00]), array([0.00000000e+00, 4.11627794e-06, 4.53171855e-04, ...,\n",
      "       2.68380338e-05, 4.24920592e-05, 1.00890374e-03]), array([1.25191005e-05, 1.85108956e-03, 0.00000000e+00, ...,\n",
      "       7.25744449e-05, 0.00000000e+00, 0.00000000e+00])]\n",
      "['across', 'action', 'actually', 'add', 'additional', 'addressing', 'administered', 'advice', 'advisory', 'aerial']\n"
     ]
    }
   ],
   "source": [
    "print(output_content)\n",
    "print(output_content_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_content_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '101',\n",
       " '109',\n",
       " '11',\n",
       " '12',\n",
       " '120',\n",
       " '13',\n",
       " '130',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '160',\n",
       " '17',\n",
       " '18',\n",
       " '180',\n",
       " '19',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2010',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '230',\n",
       " '24',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '32',\n",
       " '327',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '407',\n",
       " '41',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '4747',\n",
       " '48',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '52',\n",
       " '55',\n",
       " '60',\n",
       " '600',\n",
       " '6km',\n",
       " '70',\n",
       " '700',\n",
       " '80',\n",
       " '800',\n",
       " '888',\n",
       " '90',\n",
       " '9news',\n",
       " '_4',\n",
       " '__',\n",
       " '___',\n",
       " '____',\n",
       " '__f',\n",
       " 'aap',\n",
       " 'abc',\n",
       " 'abdullah',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abt',\n",
       " 'abuja',\n",
       " 'access',\n",
       " 'according',\n",
       " 'across',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'additional',\n",
       " 'addressing',\n",
       " 'administered',\n",
       " 'advice',\n",
       " 'advisory',\n",
       " 'aerial',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affectees',\n",
       " 'afp',\n",
       " 'africa',\n",
       " 'after',\n",
       " 'aftermath',\n",
       " 'aftershock',\n",
       " 'aftershocks',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'ago',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'al',\n",
       " 'alaska',\n",
       " 'alert',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amid',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angeles',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'announced',\n",
       " 'announces',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'ap',\n",
       " 'app',\n",
       " 'appeal',\n",
       " 'appeals',\n",
       " 'approaches',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'armed',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'as',\n",
       " 'ashore',\n",
       " 'asia',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'assam',\n",
       " 'assess',\n",
       " 'assessment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'at',\n",
       " 'atlanta',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'aur',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'authorities',\n",
       " 'available',\n",
       " 'avalanche',\n",
       " 'awake',\n",
       " 'awaran',\n",
       " 'away',\n",
       " 'awful',\n",
       " 'azad',\n",
       " 'azadi',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'baja',\n",
       " 'baldwin',\n",
       " 'baloch',\n",
       " 'balochistan',\n",
       " 'balochistanearthquake',\n",
       " 'baltimore',\n",
       " 'baluchistan',\n",
       " 'band',\n",
       " 'bangladesh',\n",
       " 'bank',\n",
       " 'base',\n",
       " 'based',\n",
       " 'batangas',\n",
       " 'bath',\n",
       " 'battered',\n",
       " 'batters',\n",
       " 'bay',\n",
       " 'bbc',\n",
       " 'bc',\n",
       " 'bcs',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'bears',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'before',\n",
       " 'begin',\n",
       " 'begins',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'bihar',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'biotech',\n",
       " 'birthday',\n",
       " 'bisbee',\n",
       " 'bit',\n",
       " 'bjp',\n",
       " 'bla',\n",
       " 'blankets',\n",
       " 'blast',\n",
       " 'blazed',\n",
       " 'bless',\n",
       " 'blog',\n",
       " 'blood',\n",
       " 'boat',\n",
       " 'boats',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'bomb',\n",
       " 'border',\n",
       " 'borongan',\n",
       " 'both',\n",
       " 'boy',\n",
       " 'braces',\n",
       " 'brampton',\n",
       " 'breaking',\n",
       " 'breakingnews',\n",
       " 'bridge',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'british',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'buckled',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'built',\n",
       " 'buried',\n",
       " 'bus',\n",
       " 'bushes',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'cabo',\n",
       " 'cabos',\n",
       " 'cabosanlucas',\n",
       " 'calamity',\n",
       " 'calif',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'camel',\n",
       " 'camels',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'camps',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'canadians',\n",
       " 'cancelled',\n",
       " 'cant',\n",
       " 'canyon',\n",
       " 'capital',\n",
       " 'capsizes',\n",
       " 'car',\n",
       " 'care',\n",
       " 'carroll',\n",
       " 'carrying',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'casualties',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'category',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'cbs',\n",
       " 'cc',\n",
       " 'cdc',\n",
       " 'cdcchat',\n",
       " 'cebu',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'charity',\n",
       " 'check',\n",
       " 'checkout',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chile',\n",
       " 'chilean',\n",
       " 'chileearthquake',\n",
       " 'chiles',\n",
       " 'china',\n",
       " 'christian',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'cities',\n",
       " 'citizen',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'cleanup',\n",
       " 'clear',\n",
       " 'click',\n",
       " 'climate',\n",
       " 'climatechange',\n",
       " 'climb',\n",
       " 'climbers',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'cm',\n",
       " 'cnn',\n",
       " 'cnns',\n",
       " 'co',\n",
       " 'coast',\n",
       " 'coastal',\n",
       " 'cold',\n",
       " 'collapsed',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colorado',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'commercial',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communication',\n",
       " 'communities',\n",
       " 'community',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerns',\n",
       " 'conditions',\n",
       " 'condolences',\n",
       " 'conference',\n",
       " 'confirmed',\n",
       " 'confirms',\n",
       " 'congress',\n",
       " 'consumption',\n",
       " 'cont',\n",
       " 'contact',\n",
       " 'contagious',\n",
       " 'continue',\n",
       " 'continues',\n",
       " 'contribute',\n",
       " 'control',\n",
       " 'cop20',\n",
       " 'corona',\n",
       " 'coronavirus',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'county',\n",
       " 'courtesy',\n",
       " 'cov',\n",
       " 'coverage',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'crisis',\n",
       " 'critical',\n",
       " 'critically',\n",
       " 'crop',\n",
       " 'crops',\n",
       " 'crore',\n",
       " 'cross',\n",
       " 'cry',\n",
       " 'cure',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cut',\n",
       " 'cyclone',\n",
       " 'cyclonepam',\n",
       " 'cyclones',\n",
       " 'daily',\n",
       " 'dallas',\n",
       " 'dam',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'damn',\n",
       " 'dams',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'data',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deadliest',\n",
       " 'deadly',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'deaths',\n",
       " 'debris',\n",
       " 'dec',\n",
       " 'decades',\n",
       " 'december',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'declares',\n",
       " 'deep',\n",
       " 'deepest',\n",
       " 'delhi',\n",
       " 'department',\n",
       " 'departments',\n",
       " 'deployed',\n",
       " 'depth',\n",
       " 'desperate',\n",
       " 'despite',\n",
       " 'destroyed',\n",
       " 'destruction',\n",
       " 'details',\n",
       " 'detectives',\n",
       " 'devastated',\n",
       " 'devastates',\n",
       " 'devastating',\n",
       " 'devastation',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diego',\n",
       " 'dies',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'disaster',\n",
       " 'disasters',\n",
       " 'discuss',\n",
       " 'disease',\n",
       " 'diseases',\n",
       " 'dispatched',\n",
       " 'displaced',\n",
       " 'district',\n",
       " 'districts',\n",
       " 'do',\n",
       " 'doctor',\n",
       " 'doctors',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'doing',\n",
       " 'dolores',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'donated',\n",
       " 'donates',\n",
       " 'donating',\n",
       " 'donation',\n",
       " 'donations',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'down',\n",
       " 'dozens',\n",
       " 'dr',\n",
       " 'dramatic',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drone',\n",
       " 'drones',\n",
       " 'drug',\n",
       " 'dry',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'earthquake',\n",
       " 'earthquakes',\n",
       " 'east',\n",
       " 'eastern',\n",
       " 'easy',\n",
       " 'ebola',\n",
       " 'ebolaoutbreak',\n",
       " 'economic',\n",
       " 'economy',\n",
       " 'ecuador',\n",
       " 'effect',\n",
       " 'effected',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'efforts',\n",
       " 'eight',\n",
       " 'electricity',\n",
       " 'embassy',\n",
       " 'emergency',\n",
       " 'emerging',\n",
       " 'employees',\n",
       " 'en',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'epicenter',\n",
       " 'epidemic',\n",
       " 'episode',\n",
       " 'especially',\n",
       " 'estimated',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'europe',\n",
       " 'evacuate',\n",
       " 'evacuated',\n",
       " 'evacuating',\n",
       " 'evacuation',\n",
       " 'evacuations',\n",
       " 'evacuees',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'everest',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evidence',\n",
       " 'ex',\n",
       " 'except',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'experimental',\n",
       " 'expert',\n",
       " 'experts',\n",
       " 'exposed',\n",
       " 'exposure',\n",
       " 'express',\n",
       " 'extensive',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'faces',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'factsnotfear',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'fall',\n",
       " 'families',\n",
       " 'family',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fatal',\n",
       " 'father',\n",
       " 'fault',\n",
       " 'fb',\n",
       " 'fear',\n",
       " 'feared',\n",
       " 'fears',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feet',\n",
       " 'felt',\n",
       " 'female',\n",
       " 'fever',\n",
       " 'few',\n",
       " 'ff',\n",
       " 'fight',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'finds',\n",
       " 'fine',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'fitness',\n",
       " 'five',\n",
       " 'fl',\n",
       " 'flash',\n",
       " 'flee',\n",
       " 'flight',\n",
       " 'flights',\n",
       " 'flood',\n",
       " 'flooded',\n",
       " 'flooding',\n",
       " 'floods',\n",
       " 'florida',\n",
       " 'flown',\n",
       " 'flu',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'followme',\n",
       " 'food',\n",
       " 'footage',\n",
       " 'for',\n",
       " 'force',\n",
       " 'forced',\n",
       " 'forces',\n",
       " 'forecast',\n",
       " 'foreign',\n",
       " 'foreigners',\n",
       " 'forget',\n",
       " 'formed',\n",
       " 'former',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'foundation',\n",
       " 'four',\n",
       " 'fox',\n",
       " 'france',\n",
       " 'francisco',\n",
       " 'free',\n",
       " 'frequently',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frm',\n",
       " 'from',\n",
       " 'front',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fucking',\n",
       " 'fukuda',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'fund',\n",
       " 'funds',\n",
       " 'funny',\n",
       " 'further',\n",
       " 'fury',\n",
       " 'future',\n",
       " 'gas',\n",
       " 'general',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'geysers',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'global',\n",
       " 'gmt',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goods',\n",
       " 'google',\n",
       " 'gorkha',\n",
       " 'got',\n",
       " 'gov',\n",
       " 'government',\n",
       " 'govt',\n",
       " 'grande',\n",
       " 'great',\n",
       " 'green',\n",
       " 'grim',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'growing',\n",
       " 'guess',\n",
       " 'guests',\n",
       " 'guidelines',\n",
       " 'guinea',\n",
       " 'gupta',\n",
       " 'guys',\n",
       " 'gwadar',\n",
       " 'h_',\n",
       " 'had',\n",
       " 'hagupit',\n",
       " 'hair',\n",
       " 'haiyan',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'havoc',\n",
       " 'hawaii',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'heads',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heartbreaking',\n",
       " 'hearts',\n",
       " 'heavy',\n",
       " 'held',\n",
       " 'helicopter',\n",
       " 'helicopters',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'helplessness',\n",
       " 'helpline',\n",
       " 'helps',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hindu',\n",
       " 'hindustan',\n",
       " 'his',\n",
       " 'historic',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hmrd',\n",
       " 'hold',\n",
       " 'holds',\n",
       " 'holidaymakers',\n",
       " 'home',\n",
       " 'homeless',\n",
       " 'homes',\n",
       " 'hope',\n",
       " 'hoping',\n",
       " 'hospital',\n",
       " 'hospitalised',\n",
       " 'hospitals',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hotels',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'houses',\n",
       " 'how',\n",
       " 'hrs',\n",
       " 'ht',\n",
       " 'ht_',\n",
       " 'htt',\n",
       " 'htt_',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'humanitarian',\n",
       " 'humanity',\n",
       " 'humans',\n",
       " 'hundreds',\n",
       " 'hurricane',\n",
       " 'hurricaneodile',\n",
       " 'hurt',\n",
       " 'husband',\n",
       " 'hyderabadmassacre',\n",
       " 'iaf',\n",
       " 'icymi',\n",
       " 'identified',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'im',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'immediate',\n",
       " 'impact',\n",
       " 'impacted',\n",
       " 'important',\n",
       " 'imported',\n",
       " 'in',\n",
       " 'include',\n",
       " 'including',\n",
       " 'india',\n",
       " 'indian',\n",
       " 'indiana',\n",
       " 'indiannews',\n",
       " 'indians',\n",
       " 'indiawithnepal',\n",
       " 'indonesia',\n",
       " 'industry',\n",
       " 'infected',\n",
       " 'infection',\n",
       " 'infectious',\n",
       " 'info',\n",
       " 'information',\n",
       " 'initial',\n",
       " 'injured',\n",
       " 'injures',\n",
       " 'injuries',\n",
       " 'inside',\n",
       " 'instagram',\n",
       " 'instead',\n",
       " 'insurance',\n",
       " 'intense',\n",
       " 'interesting',\n",
       " 'interior',\n",
       " 'international',\n",
       " 'into',\n",
       " 'involved',\n",
       " 'iquique',\n",
       " 'iran',\n",
       " 'iraq',\n",
       " 'is',\n",
       " 'islamabad',\n",
       " 'island',\n",
       " 'islands',\n",
       " 'isn',\n",
       " 'isolation',\n",
       " 'israel',\n",
       " 'issued',\n",
       " 'issues',\n",
       " 'it',\n",
       " 'items',\n",
       " 'itravel',\n",
       " 'its',\n",
       " 'jammu',\n",
       " 'japan',\n",
       " 'jawbone',\n",
       " 'jeremy',\n",
       " 'jesus',\n",
       " 'jhang',\n",
       " 'jim',\n",
       " 'jk',\n",
       " 'job',\n",
       " 'john',\n",
       " 'join',\n",
       " 'joint',\n",
       " 'jolts',\n",
       " 'journal',\n",
       " 'jtwc',\n",
       " 'june',\n",
       " 'just',\n",
       " 'kar',\n",
       " 'karachi',\n",
       " 'kashmir',\n",
       " 'kashmirflood',\n",
       " 'kashmirfloods',\n",
       " 'kashmiri',\n",
       " 'kashmiris',\n",
       " 'kathmandu',\n",
       " 'kay',\n",
       " 'keep',\n",
       " 'keeping',\n",
       " 'key',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killer',\n",
       " 'killing',\n",
       " 'kills',\n",
       " 'kindly',\n",
       " 'kits',\n",
       " 'kkf',\n",
       " 'km',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'kph',\n",
       " 'kpk',\n",
       " 'ksa',\n",
       " 'la',\n",
       " 'lack',\n",
       " 'lagos',\n",
       " 'lahore',\n",
       " 'lakh',\n",
       " 'lakhs',\n",
       " 'land',\n",
       " 'landfall',\n",
       " 'landslide',\n",
       " 'landslides',\n",
       " 'lane',\n",
       " 'large',\n",
       " 'largest',\n",
       " 'lashes',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launched',\n",
       " 'launches',\n",
       " 'leaders',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaves',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'legazpi',\n",
       " 'less',\n",
       " 'lessons',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_content_feature_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Twitter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source data from https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data presented by:\n",
    "- Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portoro, Slovenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    # eq - earthquake\n",
    "\n",
    "    print('Start dataset loading: ')\n",
    "    eq_pakistan_2013 = pd.read_csv('data/2013_pakistan_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_pakistan_2013['cat'] = 'eq'\n",
    "\n",
    "    eq_california_2014 = pd.read_csv('data/2014_california_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_california_2014['cat'] = 'eq'\n",
    "\n",
    "    eq_chile_2014 = pd.read_csv('data/2014_chile_eq_en.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_chile_2014['cat'] = 'eq'\n",
    "\n",
    "    ebola_virus_2014 = pd.read_csv('data/2014_ebola_virus.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    ebola_virus_2014['cat'] = 'virus'\n",
    "\n",
    "    hurricane_odile_2014 = pd.read_csv('data/2014_hurricane_odile.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    hurricane_odile_2014['cat'] = 'storm'\n",
    "\n",
    "    flood_india_2014 = pd.read_csv('data/2014_india_floods.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    flood_india_2014['cat'] = 'flood'\n",
    "\n",
    "    middle_east_respiratory_2014 = pd.read_csv('data/2014_mers_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    middle_east_respiratory_2014['cat'] = 'virus'\n",
    "\n",
    "    flood_pakistan_2014 = pd.read_csv('data/2014_pakistan_floods_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    flood_pakistan_2014['cat'] = 'flood'\n",
    "\n",
    "    typhoon_philippines_2014 = pd.read_csv('data/2014_typhoon_hagupit_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    typhoon_philippines_2014['cat']='storm'\n",
    "\n",
    "    cyclone_pam_2015 = pd.read_csv('data/2015_cyclone_pam_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    cyclone_pam_2015['cat'] = 'storm'\n",
    "\n",
    "    eq_nepal_2015 = pd.read_csv('data/2015_nepal_eq_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_nepal_2015['cat'] = 'eq'\n",
    "    \n",
    "    #make it one dataframe\n",
    "    df = pd.concat([cyclone_pam_2015, eq_nepal_2015, typhoon_philippines_2014, flood_pakistan_2014,\n",
    "                middle_east_respiratory_2014, flood_india_2014, hurricane_odile_2014, \n",
    "                ebola_virus_2014, eq_chile_2014, eq_california_2014, eq_pakistan_2013])\n",
    "    \n",
    "    #drop unneccessary columns\n",
    "    df = df.drop(['_unit_id', '_golden', '_trusted_judgments',\n",
    "       '_last_judgment_at', 'choose_one_category:confidence', 'choose_one_category_gold',\n",
    "       'tweet_id'], axis=1)\n",
    "    \n",
    "    #drop nan values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print('Datasets ready to use!')\n",
    "    \n",
    "    print('Total Length of DataFrame is', len(df))\n",
    "    \n",
    "    print('\\n Study the choose_one_category')\n",
    "    display(df.groupby(['choose_one_category']).count())\n",
    "    \n",
    "    print('\\n Study the cat')\n",
    "    display(df.groupby(['cat']).count())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner (dfc, hashtag_just_sign=True, remove_stopwords=True, test_position=4):  \n",
    "    #check one example before cleaning\n",
    "    print('example before cleaning at position', test_position)\n",
    "    print(dfc.iloc[test_position].tweet_text)\n",
    "\n",
    "    #IN ANY CASE \n",
    "    #non alpabetical/ numerical\n",
    "    dfc = dfc.replace(to_replace =r'&amp;', value = '', regex = True)\n",
    "    dfc = dfc.replace(to_replace =r'&gt;', value = '', regex = True)\n",
    "    #hyperlinks\n",
    "    dfc = dfc.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "    #usernames\n",
    "    dfc = dfc.replace(to_replace =r'@\\S+', value = '', regex = True) \n",
    "    #remove retweet\n",
    "    dfc = dfc.replace(to_replace ='RT :', value = '', regex = True) \n",
    "    dfc = dfc.replace(to_replace ='RT ', value = '', regex = True)\n",
    "    \n",
    "    \n",
    "    #HASHTAG OPTIONAL REMOVE\n",
    "    if(hashtag_just_sign == True):\n",
    "        dfc = dfc.replace(to_replace ='#', value = '', regex = True) \n",
    "    else:\n",
    "        dfc = dfc.replace(to_replace =r'#\\S+', value = '', regex = True)\n",
    "        \n",
    "    #IN ANY CASE\n",
    "    #remove punctation\n",
    "    dfc = dfc.replace(to_replace ='[\",:!?\\\\-]', value = ' ', regex = True)\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    dfc.tweet_text = dfc.tweet_text.str.lower()\n",
    "    #make sure no weird letters left\n",
    "    \n",
    "    #u = df.select_dtypes(object)\n",
    "    dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    \n",
    "    if(remove_stopwords == True):\n",
    "        dfc.tweet_text = dfc.tweet_text.str.split() \n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: [item for item in x if item not in eng_stopwords])\n",
    "        #join the list items back to one string\n",
    "        dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('example after cleaning at position', test_position)\n",
    "    print(dfc.iloc[test_position].tweet_text)\n",
    "    \n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=df_clean.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def training_on_content(file):\n",
    "    content = list(file.choose_one_category.unique())\n",
    "    content_importances = list()\n",
    "    content_feature_names = list()\n",
    "\n",
    "    for con in content:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = content.copy()\n",
    "        replacing_list.remove(con)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ con)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.choose_one_category = temp_file.choose_one_category.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', con)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['choose_one_category'])\n",
    "        content_feature_names.append(feature_names_out)\n",
    "        content_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Content!!!')\n",
    "    \n",
    "    return content, content_importances, content_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def training_on_cat(file):    \n",
    "    category = list(file.cat.unique())\n",
    "    cat_importances = list()\n",
    "    cat_feature_names = list()\n",
    "\n",
    "    for cat in category:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = category.copy()\n",
    "        replacing_list.remove(cat)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ cat)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.cat = temp_file.cat.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', cat)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['cat'])\n",
    "        cat_feature_names.append(feature_names_out)\n",
    "        cat_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Cat!!!')\n",
    "    \n",
    "    return category, cat_importances, cat_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_forest(data, y):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    data, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    print('confusion matrix')\n",
    "    print(metrics.confusion_matrix(y_test,test_predictions))\n",
    "    \n",
    "    print('check classes on which we trained')\n",
    "    print(forest.classes_)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    # returns relative importance of all features.\n",
    "    # they are in the order of the columns\n",
    "    print(importances)\n",
    "    len(importances)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    # sort importance scores\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    top_50 = indices[:50]\n",
    "    top50_features = [vectorizer.get_feature_names()[ind] for ind in top_50]\n",
    "    print(top50_features)\n",
    "    \n",
    "    return(forest, vectorizer, feature_names, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i am in the chicago defender newspaper. click on the link below.  '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(df_clean.tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9727802981205443 \n",
      " The validation accuracy is:  0.6925901922661482\n",
      "confusion matrix\n",
      "[[ 125    0    0   10    4    0    0    0    0    0    0   18    1    0\n",
      "     8]\n",
      " [   0   61    0    0    0    0    6    3    1    0   13  116    0   14\n",
      "     0]\n",
      " [   0    0    5    1    0    0    0    0    1    0    0    2    1    0\n",
      "     2]\n",
      " [  15    0    0   56    2    0    0    0    0    0    0   11    0    0\n",
      "     2]\n",
      " [   2    0    0    1   60    0    0    0    0    0    0   19    2    0\n",
      "     1]\n",
      " [   0    3    0    0    0   50   13    8    6    2    0   39    0    0\n",
      "     0]\n",
      " [   0    1    0    0    0    1  400    3    1    1   22   95    0    7\n",
      "     1]\n",
      " [   0    2    0    0    0    3    9  249    6    1   10   96    0    0\n",
      "     0]\n",
      " [   0    0    0    0    0    0    9    4  422    2    5   38    0    2\n",
      "     0]\n",
      " [   0    0    0    0    0    6   15    4    5   20    2   35    0    3\n",
      "     0]\n",
      " [   0    3    0    0    0    0   19    8   12    1  305  133    3   16\n",
      "     0]\n",
      " [   4   30    2    3    8    6   83   65   10    1   75 1080    6   24\n",
      "     8]\n",
      " [   1    0    0    1    1    0    0    0    0    0    2   21   32    0\n",
      "     2]\n",
      " [   0    2    0    0    0    0   44    5    6    0   29   49    0  282\n",
      "     0]\n",
      " [   3    0    0    5    1    0    0    0    0    0    3   10    2    0\n",
      "    59]]\n",
      "check classes on which we trained\n",
      "['affected_people' 'caution_and_advice' 'deaths_reports'\n",
      " 'disease_signs_or_symptoms' 'disease_transmission'\n",
      " 'displaced_people_and_evacuations'\n",
      " 'donation_needs_or_offers_or_volunteering_services'\n",
      " 'infrastructure_and_utilities_damage' 'injured_or_dead_people'\n",
      " 'missing_trapped_or_found_people' 'not_related_or_irrelevant'\n",
      " 'other_useful_information' 'prevention' 'sympathy_and_emotional_support'\n",
      " 'treatment']\n",
      "[1.99695354e-04 1.68194762e-03 1.19146595e-04 ... 1.71216948e-04\n",
      " 9.27188349e-05 1.12584282e-04]\n",
      "Feature ranking:\n",
      "['dead', 'help', 'symptoms', 'killed', 'relief', 'toll', 'damage', 'earthquake', 'prayers', 'in', 'to', 'of', 'ebola', 'the', 'nepal', 'prayforchile', 'for', 'up', 'treatment', 'india', 'mers', 'pray', 'death', 'disease', 'and', 'transmission', 'aid', 'odile', 'prevention', 'injured', 'floods', 'donate', 'is', 'california', 'victims', 'case', 'on', 'as', 'you', 'thoughts', 'from', 'hurricane', 'people', 'dozens', 'destruction', 'after', 'my', 'cabo', 'by', 'news']\n"
     ]
    }
   ],
   "source": [
    "forest, vectorizer, feature_names, importances = train_forest(df_clean.tweet_text, df_clean.choose_one_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../service_NLP/models/vectorizer.jb']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import tqdm\n",
    "from nltk.corpus import words\n",
    "\n",
    "joblib.dump(forest, '../service_NLP/models/rf_model.jb')\n",
    "joblib.dump(vectorizer, '../service_NLP/models/vectorizer.jb')\n",
    "\n",
    "dtf = pd.DataFrame(np.vstack((feature_names, importances))).transpose()\n",
    "dtf.columns = ['feature', 'importance']\n",
    "dtf.set_index('feature', inplace=True)\n",
    "msk = np.asarray([np.any([c.isdigit() for c in w]) for w in dtf.index])\n",
    "dtf = dtf[~msk]\n",
    "exi = np.asarray([w in set(words.words()) for w in tqdm.tqdm(dtf.index)])\n",
    "dtf[exi].to_parquet('../service_NLP/models/vocabulary.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ashore</th>\n",
       "      <td>0.00010011058524540075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commercial</th>\n",
       "      <td>0.00010017851613555258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct</th>\n",
       "      <td>0.00010025106757480083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>na</th>\n",
       "      <td>0.00010033536939897566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>believe</th>\n",
       "      <td>0.0001007370300095989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raising</th>\n",
       "      <td>0.00010074953478039886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick</th>\n",
       "      <td>0.00010110305404718488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0.00010130978739550051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.00010191540101408698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talking</th>\n",
       "      <td>0.00010201410924213335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrive</th>\n",
       "      <td>0.00010234509491218412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directly</th>\n",
       "      <td>0.00010236312853317062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip</th>\n",
       "      <td>0.00010265537132312112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>0.0001026580519540597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>0.00010374710730683431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagine</th>\n",
       "      <td>0.0001038343947786938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al</th>\n",
       "      <td>0.00010387100829451717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>0.00010402181514088329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welcome</th>\n",
       "      <td>0.00010418529442840705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temporary</th>\n",
       "      <td>0.00010443119812594018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed</th>\n",
       "      <td>0.00010455278652158782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spirit</th>\n",
       "      <td>0.00010478058826307993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumption</th>\n",
       "      <td>0.00010546487566451486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mar</th>\n",
       "      <td>0.00010552009491705412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killer</th>\n",
       "      <td>0.0001057015255426136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.00010582109673107618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interesting</th>\n",
       "      <td>0.00010594279775638098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>0.00010630357848009793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fault</th>\n",
       "      <td>0.00010631197027619911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.00010652788123690543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>9.271883485527602e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw</th>\n",
       "      <td>9.291832864474627e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industry</th>\n",
       "      <td>9.300043304258482e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold</th>\n",
       "      <td>9.310974171539007e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woke</th>\n",
       "      <td>9.32018024519796e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremely</th>\n",
       "      <td>9.320403366100158e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guinea</th>\n",
       "      <td>9.341330631579064e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haven</th>\n",
       "      <td>9.360600433138344e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>general</th>\n",
       "      <td>9.374764353668623e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>9.457199569407184e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calamity</th>\n",
       "      <td>9.467948967019055e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>town</th>\n",
       "      <td>9.468027719608615e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>within</th>\n",
       "      <td>9.470987246668595e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worry</th>\n",
       "      <td>9.511838909861888e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organization</th>\n",
       "      <td>9.524254479316839e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seismic</th>\n",
       "      <td>9.539698406726847e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seek</th>\n",
       "      <td>9.635363412137684e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>9.664540404084246e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>9.67459783381244e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>9.7344426914261e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>though</th>\n",
       "      <td>9.735409581533545e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>9.791809386133988e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birthday</th>\n",
       "      <td>9.80104753675848e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member</th>\n",
       "      <td>9.807666617034712e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>9.881464524678628e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>involved</th>\n",
       "      <td>9.895912901922546e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upside</th>\n",
       "      <td>9.90169950347025e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur</th>\n",
       "      <td>9.934763422895396e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>front</th>\n",
       "      <td>9.951153750637729e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conference</th>\n",
       "      <td>9.981948067130212e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1180 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          importance\n",
       "feature                             \n",
       "ashore        0.00010011058524540075\n",
       "commercial    0.00010017851613555258\n",
       "direct        0.00010025106757480083\n",
       "na            0.00010033536939897566\n",
       "believe        0.0001007370300095989\n",
       "raising       0.00010074953478039886\n",
       "sick          0.00010110305404718488\n",
       "alone         0.00010130978739550051\n",
       "dangerous     0.00010191540101408698\n",
       "talking       0.00010201410924213335\n",
       "arrive        0.00010234509491218412\n",
       "directly      0.00010236312853317062\n",
       "trip          0.00010265537132312112\n",
       "source         0.0001026580519540597\n",
       "awful         0.00010374710730683431\n",
       "imagine        0.0001038343947786938\n",
       "al            0.00010387100829451717\n",
       "reporter      0.00010402181514088329\n",
       "welcome       0.00010418529442840705\n",
       "temporary     0.00010443119812594018\n",
       "revealed      0.00010455278652158782\n",
       "spirit        0.00010478058826307993\n",
       "consumption   0.00010546487566451486\n",
       "mar           0.00010552009491705412\n",
       "killer         0.0001057015255426136\n",
       "related       0.00010582109673107618\n",
       "interesting   0.00010594279775638098\n",
       "wish          0.00010630357848009793\n",
       "fault         0.00010631197027619911\n",
       "affect        0.00010652788123690543\n",
       "...                              ...\n",
       "zero           9.271883485527602e-05\n",
       "raw            9.291832864474627e-05\n",
       "industry       9.300043304258482e-05\n",
       "hold           9.310974171539007e-05\n",
       "woke            9.32018024519796e-05\n",
       "extremely      9.320403366100158e-05\n",
       "guinea         9.341330631579064e-05\n",
       "haven          9.360600433138344e-05\n",
       "general        9.374764353668623e-05\n",
       "saying         9.457199569407184e-05\n",
       "calamity       9.467948967019055e-05\n",
       "town           9.468027719608615e-05\n",
       "within         9.470987246668595e-05\n",
       "worry          9.511838909861888e-05\n",
       "organization   9.524254479316839e-05\n",
       "seismic        9.539698406726847e-05\n",
       "seek           9.635363412137684e-05\n",
       "wall           9.664540404084246e-05\n",
       "job             9.67459783381244e-05\n",
       "try              9.7344426914261e-05\n",
       "though         9.735409581533545e-05\n",
       "fun            9.791809386133988e-05\n",
       "birthday        9.80104753675848e-05\n",
       "member         9.807666617034712e-05\n",
       "different      9.881464524678628e-05\n",
       "involved       9.895912901922546e-05\n",
       "upside          9.90169950347025e-05\n",
       "sur            9.934763422895396e-05\n",
       "front          9.951153750637729e-05\n",
       "conference     9.981948067130212e-05\n",
       "\n",
       "[1180 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('../service_NLP/models/vocabulary.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['other_useful_information',\n",
       " 'infrastructure_and_utilities_damage',\n",
       " 'injured_or_dead_people',\n",
       " 'not_related_or_irrelevant',\n",
       " 'donation_needs_or_offers_or_volunteering_services',\n",
       " 'caution_and_advice',\n",
       " 'sympathy_and_emotional_support',\n",
       " 'missing_trapped_or_found_people',\n",
       " 'displaced_people_and_evacuations',\n",
       " 'affected_people',\n",
       " 'disease_signs_or_symptoms',\n",
       " 'prevention',\n",
       " 'disease_transmission',\n",
       " 'treatment',\n",
       " 'deaths_reports']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
